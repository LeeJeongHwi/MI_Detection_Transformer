{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef114976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Nested function ---- #\n",
    "import os\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# # ---- BWR ---- #\n",
    "# import bwr\n",
    "# import emd\n",
    "# import pywt\n",
    "# ---- Scipy ---- #\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter, freqz, filtfilt\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "# ---- PyTorch ---- #\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn.functional import softmax\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from pytorchtools import EarlyStopping\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import torchvision.ops as ops\n",
    "\n",
    "# ---- Scikit Learn ---- #\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# ---- Matplotlib ---- #\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---- Summary ---- #\n",
    "import pytorch_model_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73e72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"/data/graduate/MI_Detection_Transformer/npy_data/x_train.npy\")\n",
    "y_train = np.load(\"/data/graduate/MI_Detection_Transformer/npy_data/y_train.npy\")\n",
    "x_valid = np.load(\"/data/graduate/MI_Detection_Transformer/npy_data/x_valid.npy\")\n",
    "y_valid = np.load(\"/data/graduate/MI_Detection_Transformer/npy_data/y_valid.npy\")\n",
    "x_test = np.load(\"/data/graduate/MI_Detection_Transformer/npy_data/x_test.npy\")\n",
    "y_test = np.load(\"/data/graduate/MI_Detection_Transformer/npy_data/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e32e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e69c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, patch_size, hidden_size, num_heads, num_layers):\n",
    "        super(ViTModel, self).__init__()\n",
    "\n",
    "        self.patch_embedding = nn.Conv1d(input_size, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (500 * 10) // patch_size\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches, hidden_size))\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=256, dropout=0.2, activation=\"gelu\",norm_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers, norm=self.norm)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_channels, sequence_length)\n",
    "\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, num_patches, hidden_size)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x += self.positional_embedding\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x,)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = x.mean(dim=1)  # (batch_size, hidden_size)\n",
    "\n",
    "        # Classifier Head\n",
    "        x = self.fc(x)  # (batch_size, num_classes)\n",
    "\n",
    "        return x, x   # For visualization purposes, returning x twice\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 12\n",
    "num_classes = 5\n",
    "patch_size = 20\n",
    "hidden_size = 768\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "\n",
    "# Create ViT model\n",
    "vit_model = ViTModel(input_size, num_classes, patch_size, hidden_size, num_heads, num_layers)\n",
    "vit_model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def get_DataLoader(x, y, batch, num_workers, shuffle=False):\n",
    "    x_tensor = torch.FloatTensor(x)\n",
    "    y_tensor = torch.FloatTensor(y)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch, num_workers=num_workers, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "train_loader = get_DataLoader(x_train, y_train, batch=batch_size, num_workers=num_workers, shuffle=False)\n",
    "val_loader = get_DataLoader(x_valid, y_valid, batch=16, num_workers=num_workers, shuffle=False)\n",
    "test_loader = get_DataLoader(x_test, y_test, batch=16, num_workers=num_workers, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a894707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[1/100] loss: 0.507: 100%|████████████████████████████████| 219/219 [00:32<00:00,  6.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 61.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.5359410975621716\n",
      "Epoch 1/100, Loss: 0.5294206598305811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[2/100] loss: 0.483: 100%|████████████████████████████████| 219/219 [00:33<00:00,  6.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 59.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.48839754410530334\n",
      "Epoch 2/100, Loss: 0.4774655375850799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[3/100] loss: 0.482: 100%|████████████████████████████████| 219/219 [00:33<00:00,  6.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 57.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.46701368750774697\n",
      "Epoch 3/100, Loss: 0.4427006707343881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[4/100] loss: 0.460: 100%|████████████████████████████████| 219/219 [00:38<00:00,  5.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:04<00:00, 53.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.46277126030290505\n",
      "Epoch 4/100, Loss: 0.4321748083584929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[5/100] loss: 0.442: 100%|████████████████████████████████| 219/219 [00:38<00:00,  5.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 55.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.4572925434264963\n",
      "Epoch 5/100, Loss: 0.4274387616817265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[6/100] loss: 0.442: 100%|████████████████████████████████| 219/219 [00:39<00:00,  5.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:04<00:00, 50.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.45163294652553454\n",
      "Epoch 6/100, Loss: 0.42103462554004095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[7/100] loss: 0.425: 100%|████████████████████████████████| 219/219 [00:38<00:00,  5.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 55.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.4508969108127568\n",
      "Epoch 7/100, Loss: 0.4125208631498084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[8/100] loss: 0.422: 100%|████████████████████████████████| 219/219 [00:37<00:00,  5.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:04<00:00, 51.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Decrease.. Best Model, Best Loss update\n",
      "Validation loss : 0.44787279873678126\n",
      "Epoch 8/100, Loss: 0.4053074460323543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[9/100] loss: 0.397: 100%|████████████████████████████████| 219/219 [00:38<00:00,  5.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 55.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.3991586858551252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[10/100] loss: 0.390: 100%|███████████████████████████████| 219/219 [00:37<00:00,  5.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 55.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.3956008931137111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[11/100] loss: 0.391: 100%|███████████████████████████████| 219/219 [00:36<00:00,  5.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:03<00:00, 55.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.39100658356054735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[12/100] loss: 0.363: 100%|███████████████████████████████| 219/219 [00:38<00:00,  5.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:04<00:00, 53.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.3864682842063033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[13/100] loss: 0.356: 100%|███████████████████████████████| 219/219 [00:39<00:00,  5.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 219/219 [00:04<00:00, 54.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.38249374742377296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch[14/100] loss: 0.458:  36%|███████████▍                    | 78/219 [00:13<00:25,  5.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 38\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mdesc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, num_epochs, loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# # Visualization of attention weights (for the first batch in each epoch)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# if epoch == 0 and inputs.size(0) == batch_size:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     # Get the attention weights for the first batch\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#     plt.title(\"Attention Weights - First Attention Head, First Transformer Layer\")\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#     plt.show()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "deterministic = True\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "if deterministic:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Create ViT model\n",
    "vit_model = ViTModel(input_size, num_classes, patch_size, hidden_size, num_heads, num_layers)\n",
    "vit_model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=learning_rate)\n",
    "\n",
    "compiled_model =torch.compile(vit_model)\n",
    "# Training loop with attention weights visualization\n",
    "val_loss_list = []\n",
    "best_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model.train()\n",
    "    total_loss = 0.0\n",
    "    train_bar = tqdm(train_loader)\n",
    "    for step, (inputs, labels) in enumerate(train_bar):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs, attention_weights = vit_model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_bar.desc = \"Train Epoch[{}/{}] loss: {:.3f}\".format(epoch+1, num_epochs, loss.mean().item())\n",
    "        # # Visualization of attention weights (for the first batch in each epoch)\n",
    "        # if epoch == 0 and inputs.size(0) == batch_size:\n",
    "        #     # Get the attention weights for the first batch\n",
    "        #     attention_weights = attention_weights[0, 0, 0].detach().numpy()\n",
    "        #     # Use seaborn to create a heatmap\n",
    "        #     sns.heatmap(attention_weights, cmap=\"viridis\")\n",
    "        #     plt.xlabel(\"To Patch\")\n",
    "        #     plt.ylabel(\"From Patch\")\n",
    "        #     plt.title(\"Attention Weights - First Attention Head, First Transformer Layer\")\n",
    "        #     plt.show()\n",
    "    with torch.no_grad():\n",
    "        vit_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_bar = tqdm(val_loader)\n",
    "        for v_step, (val_x, val_y) in enumerate(val_bar):\n",
    "            val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "            val_logits, at_wei = vit_model(val_x)\n",
    "            loss_v = criterion(val_logits, val_y)\n",
    "            val_loss_list.append(loss_v.mean().item())\n",
    "            val_loss += loss_v.mean().item()\n",
    "\n",
    "        print(\"Validation loss :\",val_loss/len(val_loader))\n",
    "        if best_loss > val_loss:\n",
    "            print(\"Validation Loss Decrease.. Best Model, Best Loss update\")\n",
    "            lossv = val_loss/len(val_loader)\n",
    "            best_loss = val_loss\n",
    "            # torch.save(model.state_dict(), f\"Saved_ViT_210model_{round(lossv,3) * 100}.pth\")\n",
    "            \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n",
    "\n",
    "# Save the trained model if needed\n",
    "# torch.save(vit_model.state_dict(), 'vit_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4966daf-e877-49e9-bbd2-88886fd38421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
